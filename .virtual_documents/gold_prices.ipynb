import yfinance as yf
import numpy as np
import pandas as pd
import warnings
import matplotlib
import matplotlib.pyplot as plt
from statsmodels.tsa.api import ETSModel, ExponentialSmoothing
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.ar_model import AutoReg
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.api import ETSModel, SimpleExpSmoothing, seasonal_decompose, Holt
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.ensemble import RandomForestRegressor
plt.style.use("ggplot")
plt.rcParams.update({"font.size": 14, "axes.labelweight": "bold", "lines.linewidth": 2})
import warnings

import pmdarima as pm

from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.linear_model import Ridge
from sklearn.pipeline import make_pipeline
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import FunctionTransformer
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.model_selection import train_test_split, TimeSeriesSplit, RandomizedSearchCV, cross_val_score
import lightgbm as lgb
import xgboost as xgb
from statsmodels.nonparametric.smoothers_lowess import lowess
warnings.filterwarnings("ignore",category=FutureWarning)


gold_df = yf.download('GC=F', 
                      start='2021-01-01', # YOU CAN CHANGE THIS
                      
                      end='2022-03-12', # DO NOT CHANGE THIS
                      progress=False)
gold_df['Close'].plot(title="Gold daily closing price", figsize=(12,8),ylabel='$ USD')
gold_df


days = pd.date_range('2022-03-14', '2022-03-18', freq='D')
gold_predictions = pd.DataFrame({'Date': days,
                                 'Close_predict': [np.NaN]*5})
gold_predictions= gold_predictions.set_index('Date')
gold_predictions


gold_df = yf.download(
    'GC=F', 
    start='2022-01-01',
    end='2022-03-12',
    progress=False
)


gold_df = gold_df.asfreq("b", method='pad')


gold_df.describe().round(3)


gold_df.info()


gold_df.isnull().sum()


model = seasonal_decompose(gold_df["Close"], model="additive", period=3)

with matplotlib.rc_context():
    matplotlib.rc("figure", figsize=(10, 12))
    model.plot()
    plt.tight_layout()


model = seasonal_decompose(gold_df["Close"], model="multiplicative", period=3)

with matplotlib.rc_context():
    matplotlib.rc("figure", figsize=(10, 12))
    model.plot()
    plt.tight_layout()


def stationary(df):
    """Take in a df, return log return in df_log, and first-order differencing in df_diff
       Make sure to drop na"""
    df_log, df_diff = (
        (np.log(df) - np.log(df.shift(1))).dropna(),
        df.diff().dropna()
    )

    return df_log, df_diff


def generate_plots(df, title, ylabel, lags=3):
    """Generates time, ACF, and PACF plots for a time series df"""
    fig, axes = plt.subplots(1, 3, figsize=(20, 6))

    fig.suptitle(title, y=1.02, fontsize=25)
    axes[0].plot(df)
    axes[0].set_title("Time Plot")
    axes[0].set_ylabel(ylabel)
    plot_acf(df, ax=axes[1], lags=lags)
    plot_pacf(df, ax=axes[2], lags=lags);


generate_plots(gold_df["Close"], "Gold Closing Price no Transform", "Closing Gold Price")


gold_log_df, gold_diff_df = stationary(gold_df["Close"])


generate_plots(gold_log_df, "Gold Closing Price with Log Transform", "Closing Gold Price")


generate_plots(gold_diff_df, "Gold Closing Price with 1st Order Diff", "Closing Gold Price")


# functions adapted from lecture 4
def recursive_forecast(input_data, model, n=6, responses=1):
    forecast = np.empty((n, responses))
    for i, n in enumerate(range(n)):
        forecast[i] = model.predict(input_data.reshape(1, -1))
        input_data = np.append(forecast[i], input_data[:-responses])
    return forecast.reshape((-1, responses))


def lag_df(df, lag=1, cols=None):
    if cols is None:
        cols = df.columns
    return df.assign(
        **{f"{col}-{n}": df[col].shift(n) for n in range(1, lag + 1) for col in cols}
    )


def recursive_diff_forecast(input_data, model, n=6):
    forecast = np.empty(n)
    for i, n in enumerate(range(n)):
        forecast[i] = model.predict(input_data)[-1]
        input_data = input_data.append(
            input_data.iloc[[-1]]
            .shift(1, axis=1)
            .fillna(forecast[i])
        )

    return forecast


# custom function
def get_recursive_pred(model, df, lag, index, label, diff=True):
    """Gets recursive predictions with or without differencing"""
    if diff:
        last_observations = df.iloc[-2:, :lag]

        results = pd.DataFrame(
            {
                "Close": recursive_diff_forecast(
                    last_observations,
                    model,
                    n=len(index)
                ).ravel(),
                "Label": label
            },
            index=index
        )
    else:
        last_observations = df.iloc[-1, :lag].to_numpy()

        results = pd.DataFrame(
            {
                "Close": recursive_forecast(
                    last_observations,
                    model,
                    n=len(index)
                ).ravel(),
                "Label": label
            },
            index=index
        )

    return results


# custom differencer for ML models from lecture 4
from sklearn.preprocessing import FunctionTransformer

differencer = FunctionTransformer(
    np.diff,
    inverse_func=np.cumsum,
    kw_args={"axis": 0, "prepend": 0},
    inv_kw_args={"axis": 0},
    validate=True
)


from sklearn.model_selection import train_test_split

train, test = train_test_split(
    gold_df["Close"],
    test_size=0.1,
    random_state=42,
    shuffle=False
)


train.tail()


test.head()



#code adapted from lecture notes
resid =  seasonal_decompose(train, model="additive", period=3).resid
q_01 = resid.quantile(0.1)
q_09 = resid.quantile(0.9)
upper_lim = 2 * (q_09 - q_01)
lower_lim = -2 * (q_09 - q_01)
outliers = np.where((resid > upper_lim) | (resid < lower_lim))[0]

#code adapted from 574 utils.py
def outlier_plot(df, outliers, lower_lim, upper_lim):
    fig = plt.figure(figsize=(7, 5))
    plt.plot(df)
    xlim = plt.xlim()
    plt.plot(xlim, [lower_lim] * 2, "--r")
    plt.plot(xlim, [upper_lim] * 2, "--r")
    plt.plot(df.iloc[outliers], 'ro', ms=10)
    plt.xlabel("Time"), plt.xticks(rotation=60), plt.ylabel("Weight")
outlier_plot(resid, outliers, lower_lim, upper_lim)



forecast_index = test.index
steps = len(test)


naive = pd.DataFrame(
    {
        "Close": train.iloc[-1],
        "Label": "Naive"
    },
    index=forecast_index
)


holt_wint = ExponentialSmoothing(train, trend="add", seasonal="add", seasonal_periods=3, initialization_method="estimated")
holt_wint = holt_wint.fit(method="least_squares")
holt_wint = pd.DataFrame({"Close": holt_wint.forecast(len(forecast_index)),
                        "Label": "Holt-Winters"},
                       index=forecast_index)


ets_model = ETSModel(
    train,
    error="add",
    trend="add",
    seasonal="add",
    seasonal_periods=3
).fit(disp=False)

ets = pd.DataFrame(
    {
        "Close": ets_model.forecast(steps),
        "Label": "ETS"
    },
    index=forecast_index
)


ets_model.aic



auto_ARIMA = pm.auto_arima(
    y=train,
    start_p=1,
    max_p=5,
    start_d=1,
    max_d=5,
    start_q=1,
    max_q=5,
    start_P=1,
    max_P=3,
    start_D=1,
    max_D=3,
    start_Q=1,
    max_Q=3,
    m=12,
    information_criterion="bic",
)


print(auto_ARIMA.summary())


arima = pd.DataFrame(
    {
        "Close": auto_ARIMA.predict(steps),
        "Label": "Auto_ARIMA"
    },
    index=forecast_index
)


# convert to df for ML models
train_df = pd.DataFrame(train)
test_df = pd.DataFrame(train)


#lags=3 to account for past 3 months data
lags = 3
lagged_df = lag_df(train_df, lag=lags, cols=["Close"]).dropna()

X_train = lagged_df.drop(columns=["Close"])
y_train = lagged_df["Close"]


results = {}

models = {
    "SVR_rbf": SVR(kernel="rbf"),
    "LightGBM": lgb.LGBMRegressor(),
    "XGBoost": xgb.XGBRegressor(),
}

for name, model in models.items():
    pipe = make_pipeline(differencer, model)
    fitted_pipe = pipe.fit(X_train, y_train)
    diff_label = f"{name} Recursive Diff Model"
    results[diff_label] = get_recursive_pred(fitted_pipe, lagged_df, lags, forecast_index, diff_label, True)


plt.figure(figsize=(14, 8))

# plot train and valid
plt.plot(
    train.index,
    train.sort_index(),
    label='Train',
    color="black"
)

plt.plot(test.index, test.values, label='Test')

# plot naive, ets,holt-winter and arima
plt.plot(naive.index, naive['Close'], label='Naive')
plt.plot(ets.index, ets['Close'], label='ETS')
plt.plot(holt_wint.index, holt_wint['Close'], label='Holt-Winters')
plt.plot(arima.index, arima['Close'], label='AutoARIMA')

# plot all ml models
for name, df in results.items():
    plt.plot(df.index, df['Close'], label=name)

# plot formatting
plt.title("Forcasting Gold Closing prices", fontsize=20)
plt.xlabel("Time", fontsize=12)
plt.ylabel("Unemployed Persons (000,000s)", fontsize=12)
plt.legend(loc='best')
plt.tight_layout()
plt.show();


def smape(y_true, y_pred):
    c = 2 / len(y_true) * 100
    mape = np.sum(np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))

    return c * mape


def mase(y_true, y_pred, train_diff):
    c_1 = 1 / len(y_true)
    c_2 = 1 / (len(train_diff) - 1)
    num = np.sum(np.abs(y_true - y_pred))
    denom = c_2 * np.sum(np.abs(train_diff))
    
    return c_1 * num / denom


def metric(y_train, y_pred, train_diff):
    return (smape(y_true, y_pred) + mase(y_true, y_pred, train_dff)) / 2


y_true = test
train_dff = stationary(train)[1]

mape_df = pd.DataFrame(
    [
        metric(y_true, naive['Close'], train_dff),
        metric(y_true, ets['Close'], train_dff),
        metric(y_true, holt_wint['Close'], train_dff),
        metric(y_true, arima['Close'], train_dff),
        metric(y_true, results['SVR_rbf Recursive Diff Model']['Close'], train_dff),
        metric(y_true, results['LightGBM Recursive Diff Model']['Close'], train_dff),
        metric(y_true, results['XGBoost Recursive Diff Model']['Close'], train_dff),
    ],
    columns=["Avg sMAPE/MASE"],
    index=[
        "Naive",
        "ETS",
        "Holt-Winters",
        "AutoARIMA",
        "SVR_rbf Recursive Diff Model",
        "LightGBM Recursive Diff Model",
        "XGBoost Recursive Diff Model",
    ]
)

mape_df


final_model = ETSModel(
    gold_df["Close"],
    error="add",
    trend="add",
    seasonal="add",
    seasonal_periods=3
).fit(disp=False)


days = pd.date_range('2022-03-14', '2022-03-18', freq='D')
gold_predictions = pd.DataFrame(
    {
        'Date': days,
        'Close_predict': final_model.forecast(len(days))
    }
)

gold_predictions = gold_predictions.set_index('Date')
gold_predictions



